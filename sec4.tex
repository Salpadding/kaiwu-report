\section{算法说明}

\subsection{DQN}
深度 Q 网络（Deep Q-Network，DQN）是一种基于深度学习和强化学习的算法，用于解决离散动作空间的强化学习问题。
Deep Q-learning  的目标是最小化如下的函数。

\[
J = \mathbb{E} \left[ \left( R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w) - \hat{q}(S,a,w)\right)^2 \right]
\]

这其中 $S,A,R,S'$ 分别表示当前状态，当前动作，奖励以及下一个状态的随机变量。

为了最小化目标函数，我们要使用梯度下降法，所以要对 $J(w)$ 进行求导。这里为了方便计算，假定 $\gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w)$ 中的 $w$ 是一个
固定的值，记为 $w_T$

\[
\nabla_w J= -2\mathbb{E} \left[ \left( R + \gamma \max_{a \in \mathcal{A}(S')} \hat{q}(S', a, w_T) - \hat{q}(S,a,w)\right) \nabla_w \hat{q}(S,A,w)\right]
\]



\subsubsection{算法流程}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{pic/DQN-process.png}
    \caption{\zihao{-5} DQN算法流程}
    \label{dqn-example}
\end{figure}

\subsubsection{算法伪代码}

\begin{algorithm}[H]
    \caption{DQN 算法}
    \SetAlgoLined
    \DontPrintSemicolon
      
    \textbf{Step 1} 初始化经验池为一个容量为 \( N \) 的队列 \( D \)\;
    
    \textbf{Step 2} 随机初始化 Eval 网络 \( Q \) 的权重为 \( \theta \)，Target 网络 \( Q' \) 的权重为 \( \theta' \)\;
    
    \textbf{Step 3} For episode = 1, \( M \) \textbf{do}：\;
      \quad 选取初始状态 \( S_0 \)，进行预处理得到 \( \phi_0 = \phi(S_0) \)\;
      \quad For \( t=1 \), \( T \) \textbf{do}：\;
        \quad (1) 以 \(\varepsilon\)-greedy 策略选取动作 \( a \)\;
        \quad (2) 执行动作 \( a \)，并观察得到的奖励和下一时间步的状态\;
        \quad (3) 将经验样本 \( (\phi_j, a_j, r_j, \phi_{j+1}) \) 存入经验池 \( D \)\;
        \quad (4) 从 \( D \) 中随机抽取样本 \( (\phi_j, a_j, r_j, \phi_{j+1}) \) 计算 TD target：
        \[
        y_j = 
        \begin{cases} 
        r_j, & \text{if episode terminates at step } j + 1 \\
        r_j + \gamma \max_{a'} Q'(\phi_{j+1}, a'; \theta'), & \text{otherwise}
        \end{cases}
        \]
        \quad 对 \( (y_j - Q(\phi_j, a_j; \theta))^2 \) 执行梯度下降，并更新网络 \( Q \) 的参数\;
        \quad 每 \( C \) 步将网络 \( Q \) 的参数复制给 \( Q' \)\;
      \quad \textbf{End for}\;
    \textbf{End for}
    \end{algorithm}


\subsubsection{算法代码}

\begin{enumerate}
    \item 智能体


预测的时候，采用epsilon贪心进行预测。
\begin{lstlisting}[language=Python]
    # epsilon greedy
    if not exploit_flag and np.random.rand(1) < self.epsilon:
        random_action = np.random.rand(batch, self.act_shape)
        random_action = torch.tensor(random_action, dtype=torch.float32).to(self.device)
        random_action = random_action.masked_fill(~legal_act, 0)
        act = random_action.argmax(dim=1).cpu().view(-1, 1).tolist()
    else:
        feature = [
            self.__convert_to_tensor(feature_vec),
            self.__convert_to_tensor(feature_map).view(batch, *self.obs_split[1]),
        ]
        logits, _ = model(feature, state=None)
        logits = logits.masked_fill(~legal_act, float(torch.min(logits)))
        act = logits.argmax(dim=1).cpu().view(-1, 1).tolist()
\end{lstlisting}

学习的时候和预测采用一样的模型计算q\_max，再进一步计算target\_q.
\begin{lstlisting}[language=Python]
    model = getattr(self, "model")
    model.eval()
    with torch.no_grad():
        q, h = model(_batch_feature, state=None)
        q = q.masked_fill(~_batch_obs_legal, float(torch.min(q)))
        q_max = q.max(dim=1).values.detach()

    target_q = rew + self._gamma * q_max * not_done
\end{lstlisting}

\item 模型

模型概览
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{pic/DQN-model.png}
    \caption{\zihao{-5} DQN算法模型}
    \label{map}
\end{figure}


前向推理

\begin{lstlisting}[language=Python]
# Forward inference
# 前向推理
def forward(self, s, state=None, info=None):
    feature_vec, feature_maps = s[0], s[1]
    feature_maps = self.cnn_model(feature_maps)

    feature_maps = feature_maps.view(feature_maps.shape[0], -1)

    concat_feature = torch.concat([feature_vec, feature_maps], dim=1)

    logits = self.model(concat_feature)
    return logits, state
\end{lstlisting}

 \item 奖励设置

为获得更高的分数，拆解出可能提高或者降低分数的动作，并设置相应的奖励及拼接权重。

\begin{lstlisting}[language=Python]
    """
    Concatenation of rewards: Here are 10 rewards provided,
    students can concatenate as needed, and can also add new rewards themselves
    奖励的拼接: 这里提供了10个奖励, 同学们按需自行拼接, 也可以自行添加新的奖励
    """
    REWARD_CONFIG = {
        "reward_end_dist": "0.1",
        "reward_win": "0.2",
        "reward_buff_dist": "0.001",
        "reward_buff": "0.001",
        "reward_treasure_dists": "0.1",
        "reward_treasure": "0.15",
        "reward_flicker": "0.1",
        "reward_step": "-0.01",
        "reward_bump": "-0.005",
        "reward_memory": "-0.005",
    }

    reward = [
        reward_end_dist * float(REWARD_CONFIG["reward_end_dist"]),
        reward_win * float(REWARD_CONFIG["reward_win"]),
        reward_buff_dist * float(REWARD_CONFIG["reward_buff_dist"]),
        reward_buff * float(REWARD_CONFIG["reward_buff"]),
        reward_treasure_dist * float(REWARD_CONFIG["reward_treasure_dists"]),
        reward_treasure * float(REWARD_CONFIG["reward_treasure"]),
        reward_flicker * float(REWARD_CONFIG["reward_flicker"]),
        reward_step * float(REWARD_CONFIG["reward_step"]),
        reward_bump * float(REWARD_CONFIG["reward_bump"]),
        reward_memory * float(REWARD_CONFIG["reward_memory"]),
    ]
\end{lstlisting}

\end{enumerate}


\subsection{Target-DQN}

Q-Learning 中的 TD 目标值 $r + \gamma \max_{a} Q(s', a)$ 中存在 $\max$ 操作。这会引入一个正向的偏差，
因此建模两个 $Q$ 网络，一个用于选动作，一个用于评估动作

\[
r + \gamma Q^B \left( s', \arg \max_{a} Q^A(s',a) \right)
\]


\subsubsection{算法代码}

\begin{enumerate}
    
\item 智能体

预测的时候，采用epsilon贪心进行预测。这和DQN是一样的。

\begin{lstlisting}[language=Python]
    # epsilon greedy
    if not exploit_flag and np.random.rand(1) < self.epsilon:
        random_action = np.random.rand(batch, self.act_shape)
        random_action = torch.tensor(random_action, dtype=torch.float32).to(self.device)
        random_action = random_action.masked_fill(~legal_act, 0)
        act = random_action.argmax(dim=1).cpu().view(-1, 1).tolist()
    else:
        feature = [
            self.__convert_to_tensor(feature_vec),
            self.__convert_to_tensor(feature_map).view(batch, *self.obs_split[1]),
        ]
        logits, _ = model(feature, state=None)
        logits = logits.masked_fill(~legal_act, float(torch.min(logits)))
        act = logits.argmax(dim=1).cpu().view(-1, 1).tolist()
\end{lstlisting}

学习的时候和预测采用target模型来计算计算q\_max，再进一步计算target\_q.并按照指定步数更新target模型的参数。（和DQN的不同点）
\begin{lstlisting}[language=Python]

    model = getattr(self, "target_model")
    model.eval()
    with torch.no_grad():
        q, h = model(_batch_feature, state=None)
        q = q.masked_fill(~_batch_obs_legal, float(torch.min(q)))
        q_max = q.max(dim=1).values.detach()

    target_q = rew + self._gamma * q_max * not_done
\end{lstlisting}

\begin{lstlisting}[language=Python]

    # Update the target network
    # 更新target网络
    # 按照指定步数更新
    if self.train_step % self.target_update_freq == 0:
        self.update_target_q()
\end{lstlisting}

\item 模型


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{pic/target-DQN-model.png}
    \caption{\zihao{-5} target-DQN算法模型}
    \label{target-dqn-model}
\end{figure}


\item 前向推理

\begin{lstlisting}[language=Python]
    # Forward inference
    # 前向推理
    def forward(self, s, state=None, info=None):
        feature_vec, feature_maps = s[0], s[1]
        feature_maps = self.cnn_model(feature_maps)

        feature_maps = feature_maps.view(feature_maps.shape[0], -1)

        concat_feature = torch.concat([feature_vec, feature_maps], dim=1)

        logits = self.model(concat_feature)
        return logits, state
\end{lstlisting}

\item 奖励设置

为获得更高的分数，拆解出可能提高或者降低分数的动作，并设置相应的奖励及拼接权重。

\begin{lstlisting}[language=Python]
    """
    Concatenation of rewards: Here are 10 rewards provided,
    students can concatenate as needed, and can also add new rewards themselves
    奖励的拼接: 这里提供了10个奖励, 同学们按需自行拼接, 也可以自行添加新的奖励
    """
    REWARD_CONFIG = {
        "reward_end_dist": "0.1",
        "reward_win": "0.2",
        "reward_buff_dist": "0.001",
        "reward_buff": "0.001",
        "reward_treasure_dists": "0.1",
        "reward_treasure": "0.15",
        "reward_flicker": "0.1",
        "reward_step": "-0.01",
        "reward_bump": "-0.005",
        "reward_memory": "-0.005",
    }

    reward = [
        reward_end_dist * float(REWARD_CONFIG["reward_end_dist"]),
        reward_win * float(REWARD_CONFIG["reward_win"]),
        reward_buff_dist * float(REWARD_CONFIG["reward_buff_dist"]),
        reward_buff * float(REWARD_CONFIG["reward_buff"]),
        reward_treasure_dist * float(REWARD_CONFIG["reward_treasure_dists"]),
        reward_treasure * float(REWARD_CONFIG["reward_treasure"]),
        reward_flicker * float(REWARD_CONFIG["reward_flicker"]),
        reward_step * float(REWARD_CONFIG["reward_step"]),
        reward_bump * float(REWARD_CONFIG["reward_bump"]),
        reward_memory * float(REWARD_CONFIG["reward_memory"]),
    ]
\end{lstlisting}


\end{enumerate}



\subsection{DIY}

\begin{enumerate}
\item DIY1: 修改模型

\begin{lstlisting}[language=Python]
# 前向推理
def forward(self, s, state=None, info=None):
    feature_vec, feature_maps = s[0], s[1]
    feature_maps = self.cnn_model(feature_maps)

    feature_maps = feature_maps.view(feature_maps.shape[0], -1)

    concat_feature = torch.concat([feature_vec, feature_maps], dim=1)

    logits = self.model(concat_feature)

    A = self.fc_layer3_a(logits)
    V = self.fc_layer3_v(logits)
    Q = V + A - A.mean(1).view(-1, 1)  # Q值由V值和A值计算得到

    return Q, state
\end{lstlisting}


\item DIY2: 修改学习函数

\begin{lstlisting}[language=Python]
 
    target_model = getattr(self, "target_model")
    target_model.eval()

    model = getattr(self, "model")
    model.eval()

    with torch.no_grad():
        q, h = model(_batch_feature, state=None)
        max_action = q.max(1)[1].view(-1, 1)
        next_q, next_h = target_model(_batch_feature, state=None)
        next_q = next_q.masked_fill(~_batch_obs_legal, float(torch.min(next_q)))
        q_max = next_q.gather(1, max_action).detach().flatten()
\end{lstlisting}

\end{enumerate}